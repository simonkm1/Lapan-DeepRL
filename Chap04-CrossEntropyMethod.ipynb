{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py36\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "from torch import FloatTensor, LongTensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 100\n",
    "PERCENTILE = 30 # 70 for catpole\n",
    "GAMMA = 0.95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    \n",
    "    while True:\n",
    "        #print(obs)\n",
    "        # get current obervation\n",
    "        obs_v = FloatTensor([obs])\n",
    "        ## feed forward\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        #print(act_probs)\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        #print(\"action:\",action)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        #discounting\n",
    "        episode_reward += (reward * (GAMMA ** len(episode_steps)))\n",
    "        # accumulate episide steps till done\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            # save episode\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            # yield batches of requested size\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        \n",
    "        # update to next obs\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_batch=[]\n",
    "\"\"\"\n",
    "batch - new episodes\n",
    "elite_batch - rare good episodes\n",
    "\"\"\"\n",
    "def filter_batch(batch, percentile, elite_batch):\n",
    "    batch = elite_batch + batch\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    #print(rewards)\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        #print(\"example.reward\", example.reward)\n",
    "        # accumulate observations    \n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        # accumulate actiona\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "        if example.reward > 0.01:\n",
    "            elite_batch.append(example)\n",
    "        \n",
    "    #make tenors\n",
    "    train_obs_v = FloatTensor(train_obs)\n",
    "    train_act_v = LongTensor(train_act)\n",
    "    # keep 500 items\n",
    "    del(elite_batch[0:-500])\n",
    "    print(len(elite_batch))\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, target_reward, lr = 0.05):\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=lr)\n",
    "    writer = SummaryWriter(env.name if \"name\" in dir(env) else \"\")\n",
    "\n",
    "    elite_batch=[]\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        if iter_no > 2000:\n",
    "            print(\"No convergence!\")\n",
    "            break\n",
    "        \n",
    "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE, elite_batch)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.3f, reward_bound=%.3f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "    \n",
    "        if reward_m > target_reward:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63.0, 10.0, 19.0, 20.0, 9.0, 20.0, 17.0, 19.0, 23.0, 15.0, 17.0, 20.0, 18.0, 17.0, 16.0, 30.0]\n",
      "0: loss=0.687, reward_mean=20.8, reward_bound=20.0\n",
      "[14.0, 13.0, 10.0, 11.0, 12.0, 11.0, 13.0, 10.0, 11.0, 12.0, 10.0, 10.0, 11.0, 11.0, 11.0, 12.0]\n",
      "1: loss=0.554, reward_mean=11.4, reward_bound=12.0\n",
      "[19.0, 29.0, 22.0, 13.0, 23.0, 29.0, 30.0, 20.0, 24.0, 31.0, 17.0, 40.0, 16.0, 14.0, 21.0, 16.0]\n",
      "2: loss=0.610, reward_mean=22.8, reward_bound=26.5\n",
      "[42.0, 35.0, 84.0, 54.0, 54.0, 47.0, 64.0, 72.0, 29.0, 55.0, 43.0, 36.0, 38.0, 32.0, 91.0, 38.0]\n",
      "3: loss=0.539, reward_mean=50.9, reward_bound=54.5\n",
      "[38.0, 49.0, 37.0, 33.0, 34.0, 44.0, 35.0, 44.0, 41.0, 30.0, 79.0, 35.0, 51.0, 44.0, 105.0, 26.0]\n",
      "4: loss=0.527, reward_mean=45.3, reward_bound=44.0\n",
      "[28.0, 27.0, 53.0, 49.0, 33.0, 35.0, 30.0, 42.0, 34.0, 35.0, 43.0, 35.0, 46.0, 37.0, 61.0, 31.0]\n",
      "5: loss=0.448, reward_mean=38.7, reward_bound=42.5\n",
      "[38.0, 33.0, 25.0, 32.0, 36.0, 38.0, 59.0, 35.0, 35.0, 48.0, 50.0, 37.0, 41.0, 42.0, 49.0, 109.0]\n",
      "6: loss=0.390, reward_mean=44.2, reward_bound=45.0\n",
      "[41.0, 125.0, 50.0, 36.0, 57.0, 105.0, 87.0, 115.0, 89.0, 45.0, 39.0, 30.0, 115.0, 58.0, 36.0, 70.0]\n",
      "7: loss=0.312, reward_mean=68.6, reward_bound=88.0\n",
      "[92.0, 40.0, 57.0, 49.0, 64.0, 114.0, 55.0, 61.0, 43.0, 50.0, 51.0, 54.0, 46.0, 57.0, 114.0, 65.0]\n",
      "8: loss=0.319, reward_mean=63.2, reward_bound=62.5\n",
      "[68.0, 57.0, 55.0, 50.0, 88.0, 79.0, 76.0, 45.0, 79.0, 51.0, 56.0, 66.0, 90.0, 61.0, 37.0, 62.0]\n",
      "9: loss=0.310, reward_mean=63.8, reward_bound=72.0\n",
      "[195.0, 56.0, 50.0, 127.0, 66.0, 47.0, 49.0, 48.0, 99.0, 61.0, 73.0, 51.0, 138.0, 74.0, 110.0, 86.0]\n",
      "10: loss=0.238, reward_mean=83.1, reward_bound=92.5\n",
      "[133.0, 40.0, 43.0, 101.0, 54.0, 55.0, 103.0, 75.0, 54.0, 44.0, 97.0, 97.0, 44.0, 83.0, 164.0, 51.0]\n",
      "11: loss=0.253, reward_mean=77.4, reward_bound=97.0\n",
      "[47.0, 68.0, 88.0, 73.0, 90.0, 93.0, 115.0, 61.0, 117.0, 182.0, 45.0, 101.0, 51.0, 79.0, 49.0, 49.0]\n",
      "12: loss=0.199, reward_mean=81.8, reward_bound=91.5\n",
      "[46.0, 73.0, 71.0, 53.0, 98.0, 42.0, 68.0, 148.0, 53.0, 48.0, 183.0, 57.0, 45.0, 52.0, 121.0, 52.0]\n",
      "13: loss=0.197, reward_mean=75.6, reward_bound=72.0\n",
      "[61.0, 57.0, 55.0, 70.0, 102.0, 43.0, 54.0, 95.0, 54.0, 63.0, 132.0, 36.0, 46.0, 45.0, 47.0, 200.0]\n",
      "14: loss=0.241, reward_mean=72.5, reward_bound=66.5\n",
      "[60.0, 52.0, 85.0, 55.0, 47.0, 146.0, 81.0, 89.0, 60.0, 72.0, 45.0, 70.0, 46.0, 52.0, 47.0, 53.0]\n",
      "15: loss=0.201, reward_mean=66.2, reward_bound=71.0\n",
      "[54.0, 78.0, 38.0, 114.0, 48.0, 45.0, 38.0, 92.0, 73.0, 48.0, 38.0, 61.0, 74.0, 55.0, 47.0, 129.0]\n",
      "16: loss=0.160, reward_mean=64.5, reward_bound=73.5\n",
      "[50.0, 87.0, 43.0, 62.0, 56.0, 57.0, 108.0, 47.0, 46.0, 46.0, 78.0, 139.0, 79.0, 48.0, 71.0, 177.0]\n",
      "17: loss=0.190, reward_mean=74.6, reward_bound=78.5\n",
      "[37.0, 51.0, 40.0, 45.0, 63.0, 90.0, 46.0, 44.0, 52.0, 64.0, 53.0, 69.0, 55.0, 48.0, 52.0, 45.0]\n",
      "18: loss=0.134, reward_mean=53.4, reward_bound=54.0\n",
      "[49.0, 40.0, 42.0, 78.0, 43.0, 39.0, 94.0, 49.0, 73.0, 200.0, 54.0, 49.0, 52.0, 36.0, 67.0, 57.0]\n",
      "19: loss=0.167, reward_mean=63.9, reward_bound=62.0\n",
      "[39.0, 64.0, 97.0, 47.0, 54.0, 45.0, 60.0, 57.0, 59.0, 56.0, 47.0, 45.0, 49.0, 47.0, 51.0, 64.0]\n",
      "20: loss=0.190, reward_mean=55.1, reward_bound=58.0\n",
      "[40.0, 44.0, 49.0, 58.0, 47.0, 67.0, 64.0, 86.0, 65.0, 60.0, 87.0, 48.0, 36.0, 40.0, 88.0, 48.0]\n",
      "21: loss=0.109, reward_mean=57.9, reward_bound=64.5\n",
      "[50.0, 196.0, 50.0, 50.0, 55.0, 31.0, 73.0, 43.0, 63.0, 61.0, 42.0, 93.0, 41.0, 73.0, 31.0, 72.0]\n",
      "22: loss=0.168, reward_mean=64.0, reward_bound=67.5\n",
      "[87.0, 84.0, 49.0, 50.0, 40.0, 75.0, 84.0, 41.0, 164.0, 50.0, 75.0, 53.0, 200.0, 142.0, 59.0, 54.0]\n",
      "23: loss=0.215, reward_mean=81.7, reward_bound=84.0\n",
      "[81.0, 46.0, 47.0, 32.0, 52.0, 44.0, 69.0, 74.0, 64.0, 51.0, 55.0, 53.0, 54.0, 69.0, 67.0, 36.0]\n",
      "24: loss=0.151, reward_mean=55.9, reward_bound=65.5\n",
      "[43.0, 41.0, 72.0, 55.0, 56.0, 119.0, 53.0, 53.0, 45.0, 47.0, 50.0, 70.0, 51.0, 53.0, 73.0, 53.0]\n",
      "25: loss=0.127, reward_mean=58.4, reward_bound=55.5\n",
      "[101.0, 45.0, 79.0, 70.0, 40.0, 45.0, 91.0, 54.0, 45.0, 44.0, 41.0, 83.0, 49.0, 53.0, 68.0, 52.0]\n",
      "26: loss=0.144, reward_mean=60.0, reward_bound=69.0\n",
      "[193.0, 63.0, 107.0, 80.0, 109.0, 126.0, 56.0, 44.0, 53.0, 83.0, 88.0, 46.0, 51.0, 200.0, 51.0, 74.0]\n",
      "27: loss=0.153, reward_mean=89.0, reward_bound=97.5\n",
      "[36.0, 39.0, 94.0, 59.0, 38.0, 114.0, 44.0, 149.0, 102.0, 72.0, 47.0, 58.0, 55.0, 53.0, 86.0, 51.0]\n",
      "28: loss=0.169, reward_mean=68.6, reward_bound=79.0\n",
      "[49.0, 62.0, 55.0, 58.0, 85.0, 51.0, 55.0, 48.0, 75.0, 42.0, 62.0, 45.0, 51.0, 45.0, 45.0, 159.0]\n",
      "29: loss=0.124, reward_mean=61.7, reward_bound=60.0\n",
      "[59.0, 102.0, 59.0, 77.0, 42.0, 49.0, 39.0, 45.0, 53.0, 65.0, 166.0, 40.0, 115.0, 62.0, 136.0, 76.0]\n",
      "30: loss=0.174, reward_mean=74.1, reward_bound=76.5\n",
      "[44.0, 52.0, 72.0, 51.0, 161.0, 107.0, 51.0, 131.0, 67.0, 44.0, 49.0, 64.0, 82.0, 49.0, 46.0, 42.0]\n",
      "31: loss=0.195, reward_mean=69.5, reward_bound=69.5\n",
      "[70.0, 59.0, 39.0, 55.0, 153.0, 59.0, 43.0, 50.0, 111.0, 137.0, 48.0, 101.0, 50.0, 81.0, 43.0, 89.0]\n",
      "32: loss=0.153, reward_mean=74.2, reward_bound=85.0\n",
      "[41.0, 60.0, 67.0, 120.0, 60.0, 96.0, 45.0, 200.0, 53.0, 71.0, 170.0, 84.0, 123.0, 93.0, 65.0, 86.0]\n",
      "33: loss=0.175, reward_mean=89.6, reward_bound=94.5\n",
      "[39.0, 67.0, 71.0, 76.0, 38.0, 78.0, 80.0, 52.0, 192.0, 51.0, 72.0, 59.0, 42.0, 187.0, 50.0, 77.0]\n",
      "34: loss=0.153, reward_mean=76.9, reward_bound=76.5\n",
      "[200.0, 157.0, 114.0, 53.0, 45.0, 55.0, 106.0, 200.0, 79.0, 145.0, 75.0, 85.0, 66.0, 66.0, 56.0, 59.0]\n",
      "35: loss=0.188, reward_mean=97.6, reward_bound=110.0\n",
      "[61.0, 68.0, 57.0, 87.0, 73.0, 69.0, 93.0, 50.0, 171.0, 56.0, 121.0, 65.0, 60.0, 59.0, 200.0, 69.0]\n",
      "36: loss=0.157, reward_mean=84.9, reward_bound=80.0\n",
      "[59.0, 65.0, 109.0, 65.0, 69.0, 46.0, 56.0, 61.0, 57.0, 60.0, 51.0, 65.0, 118.0, 113.0, 92.0, 65.0]\n",
      "37: loss=0.125, reward_mean=71.9, reward_bound=67.0\n",
      "[143.0, 200.0, 200.0, 48.0, 39.0, 73.0, 69.0, 111.0, 83.0, 96.0, 119.0, 53.0, 91.0, 55.0, 99.0, 63.0]\n",
      "38: loss=0.173, reward_mean=96.4, reward_bound=105.0\n",
      "[70.0, 81.0, 59.0, 57.0, 200.0, 43.0, 62.0, 52.0, 74.0, 51.0, 77.0, 56.0, 51.0, 65.0, 57.0, 93.0]\n",
      "39: loss=0.204, reward_mean=71.8, reward_bound=72.0\n",
      "[83.0, 41.0, 111.0, 61.0, 67.0, 63.0, 89.0, 55.0, 71.0, 67.0, 44.0, 59.0, 41.0, 83.0, 68.0, 177.0]\n",
      "40: loss=0.139, reward_mean=73.8, reward_bound=77.0\n",
      "[48.0, 49.0, 36.0, 57.0, 72.0, 87.0, 92.0, 58.0, 52.0, 86.0, 92.0, 87.0, 137.0, 68.0, 50.0, 80.0]\n",
      "41: loss=0.108, reward_mean=71.9, reward_bound=86.5\n",
      "[54.0, 78.0, 125.0, 98.0, 53.0, 53.0, 159.0, 74.0, 83.0, 141.0, 53.0, 73.0, 131.0, 98.0, 65.0, 41.0]\n",
      "42: loss=0.163, reward_mean=86.2, reward_bound=98.0\n",
      "[51.0, 40.0, 57.0, 63.0, 46.0, 143.0, 82.0, 61.0, 200.0, 52.0, 91.0, 91.0, 58.0, 77.0, 71.0, 62.0]\n",
      "43: loss=0.175, reward_mean=77.8, reward_bound=79.5\n",
      "[61.0, 67.0, 81.0, 59.0, 57.0, 55.0, 48.0, 177.0, 157.0, 76.0, 160.0, 85.0, 116.0, 72.0, 114.0, 200.0]\n",
      "44: loss=0.145, reward_mean=99.1, reward_bound=115.0\n",
      "[59.0, 55.0, 77.0, 109.0, 177.0, 43.0, 53.0, 46.0, 126.0, 50.0, 83.0, 59.0, 74.0, 59.0, 52.0, 60.0]\n",
      "45: loss=0.128, reward_mean=73.9, reward_bound=75.5\n",
      "[89.0, 172.0, 55.0, 138.0, 63.0, 105.0, 75.0, 49.0, 99.0, 51.0, 118.0, 49.0, 113.0, 126.0, 65.0, 56.0]\n",
      "46: loss=0.144, reward_mean=88.9, reward_bound=109.0\n",
      "[91.0, 156.0, 45.0, 191.0, 156.0, 76.0, 57.0, 89.0, 52.0, 86.0, 80.0, 153.0, 54.0, 47.0, 67.0, 79.0]\n",
      "47: loss=0.119, reward_mean=92.4, reward_bound=90.0\n",
      "[195.0, 101.0, 86.0, 59.0, 63.0, 54.0, 112.0, 75.0, 197.0, 161.0, 178.0, 82.0, 63.0, 48.0, 125.0, 111.0]\n",
      "48: loss=0.144, reward_mean=106.9, reward_bound=118.5\n",
      "[49.0, 168.0, 127.0, 78.0, 49.0, 54.0, 100.0, 57.0, 75.0, 52.0, 200.0, 65.0, 107.0, 130.0, 55.0, 82.0]\n",
      "49: loss=0.144, reward_mean=90.5, reward_bound=103.5\n",
      "[163.0, 66.0, 65.0, 95.0, 80.0, 75.0, 147.0, 99.0, 86.0, 64.0, 200.0, 84.0, 53.0, 154.0, 77.0, 200.0]\n",
      "50: loss=0.170, reward_mean=106.8, reward_bound=123.0\n",
      "[67.0, 70.0, 116.0, 97.0, 133.0, 108.0, 74.0, 66.0, 106.0, 74.0, 60.0, 90.0, 130.0, 82.0, 74.0, 82.0]\n",
      "51: loss=0.113, reward_mean=89.3, reward_bound=101.5\n",
      "[80.0, 82.0, 64.0, 80.0, 200.0, 49.0, 65.0, 78.0, 80.0, 73.0, 80.0, 52.0, 58.0, 53.0, 52.0, 50.0]\n",
      "52: loss=0.110, reward_mean=74.8, reward_bound=80.0\n",
      "[83.0, 61.0, 85.0, 62.0, 48.0, 48.0, 64.0, 70.0, 115.0, 63.0, 64.0, 94.0, 64.0, 70.0, 81.0, 52.0]\n",
      "53: loss=0.070, reward_mean=70.2, reward_bound=75.5\n",
      "[66.0, 55.0, 57.0, 59.0, 76.0, 42.0, 69.0, 69.0, 56.0, 73.0, 110.0, 54.0, 115.0, 128.0, 81.0, 79.0]\n",
      "54: loss=0.060, reward_mean=74.3, reward_bound=77.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[155.0, 83.0, 51.0, 99.0, 71.0, 120.0, 100.0, 60.0, 67.0, 56.0, 59.0, 64.0, 55.0, 64.0, 71.0, 57.0]\n",
      "55: loss=0.078, reward_mean=77.0, reward_bound=77.0\n",
      "[47.0, 56.0, 103.0, 52.0, 63.0, 55.0, 46.0, 40.0, 73.0, 43.0, 79.0, 108.0, 62.0, 83.0, 56.0, 44.0]\n",
      "56: loss=0.053, reward_mean=63.1, reward_bound=68.0\n",
      "[95.0, 94.0, 91.0, 92.0, 46.0, 67.0, 74.0, 68.0, 69.0, 59.0, 70.0, 48.0, 95.0, 68.0, 42.0, 106.0]\n",
      "57: loss=0.065, reward_mean=74.0, reward_bound=91.5\n",
      "[46.0, 48.0, 54.0, 98.0, 50.0, 44.0, 177.0, 45.0, 48.0, 47.0, 68.0, 55.0, 44.0, 44.0, 104.0, 59.0]\n",
      "58: loss=0.088, reward_mean=64.4, reward_bound=57.0\n",
      "[47.0, 62.0, 76.0, 54.0, 78.0, 67.0, 61.0, 62.0, 49.0, 48.0, 48.0, 75.0, 86.0, 52.0, 57.0, 81.0]\n",
      "59: loss=0.090, reward_mean=62.7, reward_bound=71.0\n",
      "[132.0, 61.0, 55.0, 69.0, 78.0, 106.0, 70.0, 59.0, 58.0, 56.0, 85.0, 86.0, 91.0, 50.0, 74.0, 58.0]\n",
      "60: loss=0.055, reward_mean=74.2, reward_bound=81.5\n",
      "[74.0, 46.0, 82.0, 85.0, 50.0, 48.0, 57.0, 62.0, 112.0, 49.0, 82.0, 53.0, 113.0, 71.0, 42.0, 66.0]\n",
      "61: loss=0.076, reward_mean=68.2, reward_bound=78.0\n",
      "[97.0, 67.0, 200.0, 71.0, 50.0, 79.0, 78.0, 67.0, 96.0, 72.0, 70.0, 73.0, 50.0, 140.0, 62.0, 99.0]\n",
      "62: loss=0.072, reward_mean=85.7, reward_bound=87.5\n",
      "[94.0, 104.0, 64.0, 68.0, 122.0, 65.0, 68.0, 200.0, 91.0, 200.0, 200.0, 93.0, 86.0, 200.0, 76.0, 84.0]\n",
      "63: loss=0.170, reward_mean=113.4, reward_bound=113.0\n",
      "[101.0, 200.0, 79.0, 52.0, 106.0, 63.0, 192.0, 69.0, 200.0, 86.0, 166.0, 83.0, 56.0, 60.0, 96.0, 200.0]\n",
      "64: loss=0.135, reward_mean=113.1, reward_bound=136.0\n",
      "[74.0, 200.0, 75.0, 128.0, 65.0, 195.0, 200.0, 61.0, 88.0, 66.0, 200.0, 65.0, 141.0, 86.0, 81.0, 66.0]\n",
      "65: loss=0.158, reward_mean=111.9, reward_bound=134.5\n",
      "[60.0, 82.0, 74.0, 101.0, 72.0, 71.0, 200.0, 200.0, 64.0, 200.0, 78.0, 85.0, 141.0, 78.0, 92.0, 200.0]\n",
      "66: loss=0.163, reward_mean=112.4, reward_bound=121.0\n",
      "[110.0, 73.0, 86.0, 200.0, 117.0, 72.0, 57.0, 200.0, 200.0, 73.0, 81.0, 126.0, 72.0, 200.0, 80.0, 57.0]\n",
      "67: loss=0.173, reward_mean=112.8, reward_bound=121.5\n",
      "[55.0, 67.0, 100.0, 99.0, 87.0, 87.0, 81.0, 200.0, 65.0, 87.0, 200.0, 101.0, 86.0, 63.0, 60.0, 88.0]\n",
      "68: loss=0.135, reward_mean=95.4, reward_bound=93.5\n",
      "[53.0, 72.0, 66.0, 81.0, 66.0, 76.0, 93.0, 200.0, 60.0, 77.0, 73.0, 75.0, 118.0, 67.0, 54.0, 95.0]\n",
      "69: loss=0.128, reward_mean=82.9, reward_bound=79.0\n",
      "[111.0, 115.0, 86.0, 56.0, 52.0, 81.0, 161.0, 70.0, 70.0, 136.0, 112.0, 125.0, 63.0, 127.0, 82.0, 87.0]\n",
      "70: loss=0.086, reward_mean=95.9, reward_bound=113.5\n",
      "[44.0, 84.0, 76.0, 79.0, 92.0, 64.0, 74.0, 59.0, 66.0, 72.0, 91.0, 200.0, 118.0, 90.0, 75.0, 79.0]\n",
      "71: loss=0.085, reward_mean=85.2, reward_bound=87.0\n",
      "[58.0, 41.0, 58.0, 72.0, 199.0, 63.0, 85.0, 94.0, 93.0, 67.0, 54.0, 104.0, 200.0, 69.0, 84.0, 66.0]\n",
      "72: loss=0.095, reward_mean=87.9, reward_bound=89.0\n",
      "[73.0, 76.0, 65.0, 47.0, 49.0, 74.0, 119.0, 77.0, 67.0, 68.0, 59.0, 49.0, 99.0, 64.0, 80.0, 68.0]\n",
      "73: loss=0.075, reward_mean=70.9, reward_bound=75.0\n",
      "[61.0, 74.0, 77.0, 200.0, 78.0, 86.0, 75.0, 68.0, 192.0, 162.0, 83.0, 186.0, 176.0, 57.0, 97.0, 58.0]\n",
      "74: loss=0.101, reward_mean=108.1, reward_bound=129.5\n",
      "[185.0, 80.0, 112.0, 134.0, 54.0, 53.0, 101.0, 78.0, 74.0, 75.0, 58.0, 200.0, 49.0, 157.0, 72.0, 95.0]\n",
      "75: loss=0.116, reward_mean=98.6, reward_bound=106.5\n",
      "[77.0, 63.0, 71.0, 81.0, 72.0, 62.0, 101.0, 200.0, 52.0, 177.0, 131.0, 98.0, 172.0, 85.0, 78.0, 81.0]\n",
      "76: loss=0.130, reward_mean=100.1, reward_bound=99.5\n",
      "[69.0, 99.0, 100.0, 148.0, 111.0, 69.0, 200.0, 82.0, 62.0, 85.0, 200.0, 86.0, 117.0, 74.0, 200.0, 71.0]\n",
      "77: loss=0.142, reward_mean=110.8, reward_bound=114.0\n",
      "[187.0, 200.0, 200.0, 200.0, 139.0, 71.0, 87.0, 200.0, 200.0, 89.0, 135.0, 105.0, 106.0, 200.0, 90.0, 81.0]\n",
      "78: loss=0.165, reward_mean=143.1, reward_bound=200.0\n",
      "[174.0, 120.0, 200.0, 163.0, 200.0, 200.0, 87.0, 200.0, 128.0, 65.0, 184.0, 101.0, 200.0, 103.0, 125.0, 200.0]\n",
      "79: loss=0.186, reward_mean=153.1, reward_bound=200.0\n",
      "[63.0, 200.0, 111.0, 200.0, 137.0, 182.0, 66.0, 83.0, 122.0, 68.0, 116.0, 67.0, 72.0, 113.0, 85.0, 63.0]\n",
      "80: loss=0.139, reward_mean=109.2, reward_bound=119.0\n",
      "[84.0, 73.0, 167.0, 148.0, 109.0, 81.0, 68.0, 69.0, 125.0, 98.0, 172.0, 90.0, 200.0, 67.0, 76.0, 105.0]\n",
      "81: loss=0.120, reward_mean=108.2, reward_bound=117.0\n",
      "[74.0, 59.0, 90.0, 87.0, 112.0, 98.0, 200.0, 200.0, 200.0, 133.0, 200.0, 145.0, 131.0, 97.0, 67.0, 200.0]\n",
      "82: loss=0.158, reward_mean=130.8, reward_bound=172.5\n",
      "[110.0, 87.0, 113.0, 109.0, 59.0, 83.0, 200.0, 61.0, 107.0, 70.0, 110.0, 148.0, 78.0, 70.0, 124.0, 140.0]\n",
      "83: loss=0.102, reward_mean=104.3, reward_bound=111.5\n",
      "[97.0, 83.0, 170.0, 127.0, 71.0, 119.0, 67.0, 200.0, 160.0, 200.0, 77.0, 200.0, 148.0, 79.0, 200.0, 135.0]\n",
      "84: loss=0.187, reward_mean=133.3, reward_bound=165.0\n",
      "[97.0, 200.0, 61.0, 108.0, 200.0, 116.0, 167.0, 200.0, 76.0, 197.0, 90.0, 74.0, 165.0, 109.0, 146.0, 114.0]\n",
      "85: loss=0.181, reward_mean=132.5, reward_bound=166.0\n",
      "[167.0, 200.0, 147.0, 75.0, 71.0, 200.0, 71.0, 200.0, 91.0, 200.0, 74.0, 200.0, 200.0, 173.0, 200.0, 158.0]\n",
      "86: loss=0.192, reward_mean=151.7, reward_bound=200.0\n",
      "[84.0, 71.0, 102.0, 85.0, 200.0, 200.0, 76.0, 76.0, 200.0, 100.0, 66.0, 137.0, 200.0, 83.0, 165.0, 149.0]\n",
      "87: loss=0.174, reward_mean=124.6, reward_bound=157.0\n",
      "[200.0, 200.0, 114.0, 106.0, 72.0, 153.0, 103.0, 72.0, 178.0, 150.0, 132.0, 200.0, 80.0, 135.0, 200.0, 200.0]\n",
      "88: loss=0.202, reward_mean=143.4, reward_bound=189.0\n",
      "[124.0, 69.0, 77.0, 107.0, 134.0, 116.0, 200.0, 93.0, 200.0, 97.0, 94.0, 200.0, 200.0, 90.0, 200.0, 199.0]\n",
      "89: loss=0.145, reward_mean=137.5, reward_bound=199.5\n",
      "[70.0, 61.0, 129.0, 80.0, 99.0, 200.0, 200.0, 97.0, 93.0, 106.0, 82.0, 97.0, 132.0, 90.0, 89.0, 73.0]\n",
      "90: loss=0.127, reward_mean=106.1, reward_bound=102.5\n",
      "[108.0, 87.0, 98.0, 155.0, 73.0, 115.0, 108.0, 187.0, 81.0, 76.0, 57.0, 65.0, 61.0, 92.0, 126.0, 82.0]\n",
      "91: loss=0.092, reward_mean=98.2, reward_bound=108.0\n",
      "[160.0, 75.0, 200.0, 69.0, 94.0, 98.0, 79.0, 148.0, 95.0, 111.0, 89.0, 56.0, 95.0, 200.0, 94.0, 109.0]\n",
      "92: loss=0.130, reward_mean=110.8, reward_bound=110.0\n",
      "[105.0, 200.0, 200.0, 67.0, 144.0, 81.0, 61.0, 131.0, 125.0, 93.0, 200.0, 93.0, 76.0, 111.0, 74.0, 83.0]\n",
      "93: loss=0.166, reward_mean=115.2, reward_bound=128.0\n",
      "[68.0, 124.0, 81.0, 105.0, 49.0, 200.0, 120.0, 63.0, 95.0, 80.0, 108.0, 65.0, 130.0, 169.0, 87.0, 62.0]\n",
      "94: loss=0.133, reward_mean=100.4, reward_bound=114.0\n",
      "[200.0, 128.0, 120.0, 55.0, 59.0, 77.0, 91.0, 79.0, 66.0, 154.0, 108.0, 76.0, 194.0, 132.0, 110.0, 104.0]\n",
      "95: loss=0.101, reward_mean=109.6, reward_bound=124.0\n",
      "[56.0, 172.0, 51.0, 81.0, 87.0, 80.0, 60.0, 76.0, 86.0, 75.0, 93.0, 172.0, 64.0, 136.0, 61.0, 69.0]\n",
      "96: loss=0.085, reward_mean=88.7, reward_bound=86.5\n",
      "[55.0, 126.0, 57.0, 101.0, 62.0, 136.0, 82.0, 114.0, 87.0, 79.0, 61.0, 50.0, 105.0, 73.0, 71.0, 71.0]\n",
      "97: loss=0.083, reward_mean=83.1, reward_bound=94.0\n",
      "[87.0, 59.0, 200.0, 57.0, 62.0, 93.0, 189.0, 49.0, 65.0, 55.0, 90.0, 67.0, 67.0, 66.0, 200.0, 195.0]\n",
      "98: loss=0.157, reward_mean=100.1, reward_bound=91.5\n",
      "[56.0, 42.0, 84.0, 69.0, 61.0, 93.0, 192.0, 75.0, 85.0, 103.0, 88.0, 99.0, 61.0, 61.0, 71.0, 79.0]\n",
      "99: loss=0.080, reward_mean=82.4, reward_bound=86.5\n",
      "[96.0, 57.0, 169.0, 101.0, 164.0, 71.0, 58.0, 47.0, 61.0, 57.0, 77.0, 59.0, 54.0, 79.0, 58.0, 60.0]\n",
      "100: loss=0.081, reward_mean=79.2, reward_bound=78.0\n",
      "[56.0, 86.0, 69.0, 167.0, 65.0, 78.0, 49.0, 80.0, 53.0, 160.0, 80.0, 49.0, 181.0, 62.0, 67.0, 61.0]\n",
      "101: loss=0.099, reward_mean=85.2, reward_bound=80.0\n",
      "[58.0, 48.0, 64.0, 77.0, 50.0, 94.0, 96.0, 54.0, 93.0, 54.0, 64.0, 52.0, 59.0, 48.0, 52.0, 127.0]\n",
      "102: loss=0.034, reward_mean=68.1, reward_bound=70.5\n",
      "[67.0, 57.0, 81.0, 102.0, 55.0, 80.0, 84.0, 65.0, 69.0, 64.0, 81.0, 51.0, 200.0, 73.0, 52.0, 55.0]\n",
      "103: loss=0.110, reward_mean=77.2, reward_bound=80.5\n",
      "[84.0, 200.0, 119.0, 175.0, 53.0, 73.0, 62.0, 78.0, 59.0, 78.0, 61.0, 54.0, 162.0, 178.0, 68.0, 63.0]\n",
      "104: loss=0.122, reward_mean=97.9, reward_bound=101.5\n",
      "[200.0, 66.0, 52.0, 75.0, 108.0, 59.0, 193.0, 162.0, 96.0, 55.0, 137.0, 78.0, 155.0, 88.0, 66.0, 124.0]\n",
      "105: loss=0.113, reward_mean=107.1, reward_bound=130.5\n",
      "[197.0, 52.0, 68.0, 84.0, 200.0, 83.0, 74.0, 83.0, 89.0, 79.0, 79.0, 127.0, 57.0, 59.0, 182.0, 57.0]\n",
      "106: loss=0.113, reward_mean=98.1, reward_bound=86.5\n",
      "[101.0, 52.0, 98.0, 79.0, 63.0, 81.0, 73.0, 71.0, 76.0, 81.0, 59.0, 60.0, 69.0, 80.0, 172.0, 200.0]\n",
      "107: loss=0.109, reward_mean=88.4, reward_bound=81.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78.0, 65.0, 100.0, 69.0, 79.0, 131.0, 55.0, 61.0, 73.0, 83.0, 63.0, 145.0, 69.0, 63.0, 55.0, 54.0]\n",
      "108: loss=0.065, reward_mean=77.7, reward_bound=78.5\n",
      "[72.0, 90.0, 200.0, 53.0, 193.0, 79.0, 55.0, 53.0, 55.0, 200.0, 66.0, 145.0, 95.0, 79.0, 85.0, 53.0]\n",
      "109: loss=0.092, reward_mean=98.3, reward_bound=92.5\n",
      "[84.0, 136.0, 52.0, 73.0, 86.0, 81.0, 66.0, 78.0, 71.0, 186.0, 81.0, 76.0, 76.0, 83.0, 59.0, 61.0]\n",
      "110: loss=0.072, reward_mean=84.3, reward_bound=82.0\n",
      "[200.0, 59.0, 62.0, 200.0, 72.0, 161.0, 63.0, 80.0, 61.0, 70.0, 60.0, 70.0, 181.0, 70.0, 89.0, 123.0]\n",
      "111: loss=0.092, reward_mean=101.3, reward_bound=106.0\n",
      "[200.0, 68.0, 200.0, 200.0, 77.0, 74.0, 64.0, 71.0, 200.0, 64.0, 73.0, 162.0, 74.0, 98.0, 200.0, 105.0]\n",
      "112: loss=0.156, reward_mean=120.6, reward_bound=181.0\n",
      "[78.0, 97.0, 80.0, 93.0, 104.0, 90.0, 84.0, 87.0, 68.0, 81.0, 200.0, 80.0, 65.0, 83.0, 174.0, 96.0]\n",
      "113: loss=0.094, reward_mean=97.5, reward_bound=94.5\n",
      "[108.0, 67.0, 63.0, 61.0, 58.0, 62.0, 63.0, 77.0, 75.0, 77.0, 200.0, 74.0, 66.0, 96.0, 78.0, 154.0]\n",
      "114: loss=0.069, reward_mean=86.2, reward_bound=77.5\n",
      "[70.0, 94.0, 77.0, 68.0, 87.0, 58.0, 96.0, 70.0, 92.0, 74.0, 142.0, 81.0, 98.0, 56.0, 66.0, 66.0]\n",
      "115: loss=0.060, reward_mean=80.9, reward_bound=89.5\n",
      "[72.0, 200.0, 200.0, 79.0, 138.0, 65.0, 115.0, 82.0, 66.0, 62.0, 94.0, 76.0, 94.0, 59.0, 200.0, 72.0]\n",
      "116: loss=0.134, reward_mean=104.6, reward_bound=104.5\n",
      "[132.0, 68.0, 200.0, 81.0, 186.0, 79.0, 76.0, 148.0, 66.0, 96.0, 70.0, 108.0, 104.0, 200.0, 88.0, 200.0]\n",
      "117: loss=0.145, reward_mean=118.9, reward_bound=140.0\n",
      "[98.0, 84.0, 115.0, 114.0, 91.0, 122.0, 91.0, 82.0, 106.0, 80.0, 84.0, 87.0, 80.0, 98.0, 95.0, 70.0]\n",
      "118: loss=0.071, reward_mean=93.6, reward_bound=98.0\n",
      "[88.0, 80.0, 76.0, 67.0, 71.0, 69.0, 200.0, 200.0, 123.0, 67.0, 74.0, 104.0, 84.0, 76.0, 106.0, 106.0]\n",
      "119: loss=0.064, reward_mean=99.4, reward_bound=105.0\n",
      "[72.0, 71.0, 93.0, 80.0, 200.0, 88.0, 200.0, 114.0, 115.0, 184.0, 126.0, 106.0, 200.0, 87.0, 100.0, 149.0]\n",
      "120: loss=0.140, reward_mean=124.1, reward_bound=137.5\n",
      "[120.0, 149.0, 84.0, 173.0, 109.0, 109.0, 127.0, 200.0, 200.0, 77.0, 106.0, 200.0, 200.0, 178.0, 80.0, 84.0]\n",
      "121: loss=0.129, reward_mean=137.2, reward_bound=175.5\n",
      "[200.0, 113.0, 70.0, 92.0, 200.0, 70.0, 88.0, 200.0, 199.0, 118.0, 171.0, 200.0, 82.0, 200.0, 79.0, 107.0]\n",
      "122: loss=0.167, reward_mean=136.8, reward_bound=199.5\n",
      "[200.0, 139.0, 103.0, 76.0, 200.0, 106.0, 119.0, 200.0, 112.0, 87.0, 200.0, 143.0, 138.0, 98.0, 119.0, 169.0]\n",
      "123: loss=0.133, reward_mean=138.1, reward_bound=156.0\n",
      "[81.0, 76.0, 107.0, 82.0, 79.0, 73.0, 83.0, 110.0, 101.0, 84.0, 117.0, 86.0, 77.0, 81.0, 84.0, 200.0]\n",
      "124: loss=0.089, reward_mean=95.1, reward_bound=93.5\n",
      "[200.0, 117.0, 146.0, 89.0, 96.0, 200.0, 104.0, 88.0, 111.0, 85.0, 91.0, 135.0, 89.0, 80.0, 129.0, 87.0]\n",
      "125: loss=0.080, reward_mean=115.4, reward_bound=123.0\n",
      "[111.0, 85.0, 129.0, 87.0, 115.0, 200.0, 147.0, 105.0, 84.0, 85.0, 118.0, 103.0, 95.0, 99.0, 88.0, 131.0]\n",
      "126: loss=0.071, reward_mean=111.4, reward_bound=116.5\n",
      "[87.0, 128.0, 200.0, 200.0, 118.0, 92.0, 105.0, 178.0, 95.0, 96.0, 200.0, 110.0, 93.0, 200.0, 125.0, 110.0]\n",
      "127: loss=0.103, reward_mean=133.6, reward_bound=153.0\n",
      "[113.0, 101.0, 200.0, 94.0, 91.0, 101.0, 146.0, 200.0, 129.0, 187.0, 141.0, 200.0, 93.0, 200.0, 98.0, 112.0]\n",
      "128: loss=0.123, reward_mean=137.9, reward_bound=166.5\n",
      "[200.0, 94.0, 122.0, 176.0, 136.0, 118.0, 200.0, 117.0, 152.0, 104.0, 120.0, 113.0, 165.0, 107.0, 118.0, 153.0]\n",
      "129: loss=0.133, reward_mean=137.2, reward_bound=152.5\n",
      "[95.0, 92.0, 168.0, 147.0, 193.0, 107.0, 173.0, 141.0, 105.0, 200.0, 172.0, 191.0, 133.0, 200.0, 176.0, 182.0]\n",
      "130: loss=0.140, reward_mean=154.7, reward_bound=179.0\n",
      "[143.0, 130.0, 130.0, 200.0, 168.0, 102.0, 200.0, 152.0, 200.0, 178.0, 98.0, 136.0, 112.0, 135.0, 91.0, 200.0]\n",
      "131: loss=0.109, reward_mean=148.4, reward_bound=173.0\n",
      "[118.0, 146.0, 173.0, 154.0, 94.0, 104.0, 200.0, 138.0, 200.0, 99.0, 183.0, 200.0, 84.0, 108.0, 116.0, 108.0]\n",
      "132: loss=0.114, reward_mean=139.1, reward_bound=163.5\n",
      "[200.0, 153.0, 193.0, 122.0, 152.0, 174.0, 92.0, 170.0, 91.0, 97.0, 163.0, 162.0, 165.0, 97.0, 200.0, 107.0]\n",
      "133: loss=0.124, reward_mean=146.1, reward_bound=167.5\n",
      "[139.0, 183.0, 145.0, 89.0, 94.0, 152.0, 119.0, 200.0, 148.0, 142.0, 152.0, 142.0, 200.0, 195.0, 150.0, 186.0]\n",
      "134: loss=0.098, reward_mean=152.2, reward_bound=167.5\n",
      "[150.0, 106.0, 98.0, 200.0, 130.0, 200.0, 200.0, 200.0, 104.0, 200.0, 156.0, 200.0, 190.0, 138.0, 156.0, 138.0]\n",
      "135: loss=0.139, reward_mean=160.4, reward_bound=200.0\n",
      "[200.0, 200.0, 200.0, 200.0, 200.0, 155.0, 133.0, 162.0, 120.0, 200.0, 200.0, 124.0, 185.0, 200.0, 183.0, 198.0]\n",
      "136: loss=0.149, reward_mean=178.8, reward_bound=200.0\n",
      "[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 155.0, 200.0, 134.0, 200.0, 119.0, 179.0, 200.0, 200.0]\n",
      "137: loss=0.161, reward_mean=186.7, reward_bound=200.0\n",
      "[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 140.0, 166.0, 200.0, 116.0, 200.0]\n",
      "138: loss=0.170, reward_mean=188.9, reward_bound=200.0\n",
      "[200.0, 200.0, 143.0, 185.0, 200.0, 167.0, 200.0, 200.0, 193.0, 200.0, 129.0, 200.0, 200.0, 200.0, 117.0, 200.0]\n",
      "139: loss=0.184, reward_mean=183.4, reward_bound=200.0\n",
      "[200.0, 200.0, 200.0, 200.0, 200.0, 140.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n",
      "140: loss=0.170, reward_mean=196.2, reward_bound=200.0\n",
      "[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 147.0, 200.0, 200.0, 200.0, 143.0, 200.0, 200.0, 200.0]\n",
      "141: loss=0.130, reward_mean=193.1, reward_bound=200.0\n",
      "[200.0, 156.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n",
      "142: loss=0.186, reward_mean=197.2, reward_bound=200.0\n",
      "[200.0, 200.0, 170.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n",
      "143: loss=0.151, reward_mean=198.1, reward_bound=200.0\n",
      "[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 185.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n",
      "144: loss=0.160, reward_mean=199.1, reward_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\",199)\n",
    "train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "0: loss=1.344, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "1: loss=1.039, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "2: loss=0.846, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "3: loss=0.625, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "4: loss=0.491, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "5: loss=0.443, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "6: loss=0.419, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "7: loss=0.403, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "8: loss=0.404, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "9: loss=0.400, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "10: loss=0.397, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "11: loss=0.400, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "12: loss=0.401, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "13: loss=0.393, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "14: loss=0.388, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "15: loss=0.379, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "16: loss=0.379, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "17: loss=0.378, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "18: loss=0.379, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "19: loss=0.378, reward_mean=0.5, reward_bound=0.6\n",
      "500\n",
      "20: loss=0.378, reward_mean=0.5, reward_bound=0.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-e66ad6cf1188>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDiscreteOneHotWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"FrozenLake-v0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-100-1e06cf12ac89>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"name\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0miter_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterate_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miter_no\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No convergence!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-f199ec940ddd>\u001b[0m in \u001b[0;36miterate_batches\u001b[1;34m(env, net, batch_size)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mobs_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m## feed forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mact_probs_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mact_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mact_probs_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#print(act_probs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    863\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 865\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "train(env,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non slippary FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0: loss=1.382, reward_mean=0.000, reward_bound=0.000\n",
      "300\n",
      "1: loss=1.382, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "2: loss=1.382, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "3: loss=1.382, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "4: loss=1.382, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "5: loss=1.383, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "6: loss=1.384, reward_mean=0.008, reward_bound=0.000\n",
      "500\n",
      "7: loss=1.384, reward_mean=0.010, reward_bound=0.000\n",
      "500\n",
      "8: loss=1.382, reward_mean=0.011, reward_bound=0.000\n",
      "500\n",
      "9: loss=1.382, reward_mean=0.010, reward_bound=0.000\n",
      "500\n",
      "10: loss=1.382, reward_mean=0.008, reward_bound=0.000\n",
      "500\n",
      "11: loss=1.382, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "12: loss=1.380, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "13: loss=1.380, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "14: loss=1.380, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "15: loss=1.380, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "16: loss=1.380, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "17: loss=1.379, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "18: loss=1.380, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "19: loss=1.381, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "20: loss=1.382, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "21: loss=1.381, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "22: loss=1.380, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "23: loss=1.379, reward_mean=0.008, reward_bound=0.000\n",
      "500\n",
      "24: loss=1.379, reward_mean=0.008, reward_bound=0.000\n",
      "500\n",
      "25: loss=1.378, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "26: loss=1.377, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "27: loss=1.377, reward_mean=0.008, reward_bound=0.000\n",
      "500\n",
      "28: loss=1.378, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "29: loss=1.379, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "30: loss=1.377, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "31: loss=1.377, reward_mean=0.008, reward_bound=0.000\n",
      "500\n",
      "32: loss=1.377, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "33: loss=1.376, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "34: loss=1.374, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "35: loss=1.373, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "36: loss=1.375, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "37: loss=1.374, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "38: loss=1.373, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "39: loss=1.373, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "40: loss=1.374, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "41: loss=1.375, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "42: loss=1.374, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "43: loss=1.373, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "44: loss=1.374, reward_mean=0.008, reward_bound=0.000\n",
      "500\n",
      "45: loss=1.375, reward_mean=0.008, reward_bound=0.000\n",
      "500\n",
      "46: loss=1.374, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "47: loss=1.371, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "48: loss=1.371, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "49: loss=1.370, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "50: loss=1.368, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "51: loss=1.370, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "52: loss=1.368, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "53: loss=1.369, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "54: loss=1.367, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "55: loss=1.369, reward_mean=0.008, reward_bound=0.000\n",
      "500\n",
      "56: loss=1.370, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "57: loss=1.367, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "58: loss=1.366, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "59: loss=1.366, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "60: loss=1.367, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "61: loss=1.367, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "62: loss=1.367, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "63: loss=1.367, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "64: loss=1.366, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "65: loss=1.366, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "66: loss=1.365, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "67: loss=1.363, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "68: loss=1.363, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "69: loss=1.366, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "70: loss=1.369, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "71: loss=1.368, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "72: loss=1.368, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "73: loss=1.367, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "74: loss=1.366, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "75: loss=1.367, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "76: loss=1.366, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "77: loss=1.369, reward_mean=0.008, reward_bound=0.000\n",
      "500\n",
      "78: loss=1.368, reward_mean=0.010, reward_bound=0.000\n",
      "500\n",
      "79: loss=1.369, reward_mean=0.010, reward_bound=0.000\n",
      "500\n",
      "80: loss=1.371, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "81: loss=1.369, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "82: loss=1.367, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "83: loss=1.363, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "84: loss=1.365, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "85: loss=1.365, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "86: loss=1.365, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "87: loss=1.367, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "88: loss=1.368, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "89: loss=1.367, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "90: loss=1.368, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "91: loss=1.367, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "92: loss=1.366, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "93: loss=1.365, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "94: loss=1.364, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "95: loss=1.367, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "96: loss=1.365, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "97: loss=1.365, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "98: loss=1.364, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "99: loss=1.362, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "100: loss=1.363, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "101: loss=1.362, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "102: loss=1.363, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "103: loss=1.363, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "104: loss=1.362, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "105: loss=1.362, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "106: loss=1.359, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "107: loss=1.359, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "108: loss=1.360, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "109: loss=1.360, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "110: loss=1.362, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "111: loss=1.362, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "112: loss=1.360, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "113: loss=1.359, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "114: loss=1.356, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "115: loss=1.354, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "116: loss=1.352, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "117: loss=1.351, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "118: loss=1.356, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "119: loss=1.357, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "120: loss=1.359, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "121: loss=1.358, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "122: loss=1.358, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "123: loss=1.358, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "124: loss=1.356, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "125: loss=1.354, reward_mean=0.008, reward_bound=0.000\n",
      "500\n",
      "126: loss=1.355, reward_mean=0.008, reward_bound=0.000\n",
      "500\n",
      "127: loss=1.357, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "128: loss=1.355, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "129: loss=1.352, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "130: loss=1.351, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "131: loss=1.352, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "132: loss=1.350, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "133: loss=1.349, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "134: loss=1.351, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "135: loss=1.352, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "136: loss=1.350, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "137: loss=1.350, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "138: loss=1.350, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "139: loss=1.349, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "140: loss=1.349, reward_mean=0.003, reward_bound=0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "141: loss=1.350, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "142: loss=1.352, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "143: loss=1.351, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "144: loss=1.351, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "145: loss=1.350, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "146: loss=1.349, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "147: loss=1.349, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "148: loss=1.350, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "149: loss=1.347, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "150: loss=1.347, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "151: loss=1.345, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "152: loss=1.344, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "153: loss=1.343, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "154: loss=1.339, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "155: loss=1.340, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "156: loss=1.338, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "157: loss=1.338, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "158: loss=1.340, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "159: loss=1.339, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "160: loss=1.340, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "161: loss=1.336, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "162: loss=1.337, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "163: loss=1.335, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "164: loss=1.330, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "165: loss=1.332, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "166: loss=1.330, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "167: loss=1.331, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "168: loss=1.328, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "169: loss=1.331, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "170: loss=1.334, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "171: loss=1.329, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "172: loss=1.331, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "173: loss=1.329, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "174: loss=1.325, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "175: loss=1.327, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "176: loss=1.322, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "177: loss=1.320, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "178: loss=1.317, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "179: loss=1.317, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "180: loss=1.324, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "181: loss=1.318, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "182: loss=1.319, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "183: loss=1.317, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "184: loss=1.318, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "185: loss=1.319, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "186: loss=1.314, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "187: loss=1.315, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "188: loss=1.314, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "189: loss=1.315, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "190: loss=1.313, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "191: loss=1.309, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "192: loss=1.310, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "193: loss=1.310, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "194: loss=1.306, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "195: loss=1.302, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "196: loss=1.302, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "197: loss=1.307, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "198: loss=1.311, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "199: loss=1.310, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "200: loss=1.315, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "201: loss=1.316, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "202: loss=1.313, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "203: loss=1.311, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "204: loss=1.313, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "205: loss=1.313, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "206: loss=1.316, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "207: loss=1.320, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "208: loss=1.321, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "209: loss=1.321, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "210: loss=1.319, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "211: loss=1.319, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "212: loss=1.317, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "213: loss=1.319, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "214: loss=1.324, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "215: loss=1.324, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "216: loss=1.322, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "217: loss=1.325, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "218: loss=1.328, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "219: loss=1.332, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "220: loss=1.327, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "221: loss=1.328, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "222: loss=1.327, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "223: loss=1.328, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "224: loss=1.327, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "225: loss=1.323, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "226: loss=1.324, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "227: loss=1.325, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "228: loss=1.328, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "229: loss=1.326, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "230: loss=1.325, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "231: loss=1.326, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "232: loss=1.328, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "233: loss=1.328, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "234: loss=1.324, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "235: loss=1.326, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "236: loss=1.329, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "237: loss=1.327, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "238: loss=1.322, reward_mean=0.006, reward_bound=0.000\n",
      "500\n",
      "239: loss=1.321, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "240: loss=1.320, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "241: loss=1.316, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "242: loss=1.311, reward_mean=0.007, reward_bound=0.000\n",
      "500\n",
      "243: loss=1.314, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "244: loss=1.320, reward_mean=0.005, reward_bound=0.000\n",
      "500\n",
      "245: loss=1.316, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "246: loss=1.316, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "247: loss=1.315, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "248: loss=1.315, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "249: loss=1.313, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "250: loss=1.306, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "251: loss=1.309, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "252: loss=1.313, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "253: loss=1.312, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "254: loss=1.313, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "255: loss=1.309, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "256: loss=1.310, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "257: loss=1.312, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "258: loss=1.310, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "259: loss=1.314, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "260: loss=1.307, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "261: loss=1.306, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "262: loss=1.306, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "263: loss=1.304, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "264: loss=1.306, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "265: loss=1.303, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "266: loss=1.309, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "267: loss=1.315, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "268: loss=1.310, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "269: loss=1.306, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "270: loss=1.299, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "271: loss=1.298, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "272: loss=1.296, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "273: loss=1.295, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "274: loss=1.296, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "275: loss=1.299, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "276: loss=1.299, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "277: loss=1.298, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "278: loss=1.296, reward_mean=0.004, reward_bound=0.000\n",
      "500\n",
      "279: loss=1.292, reward_mean=0.003, reward_bound=0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "280: loss=1.290, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "281: loss=1.284, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "282: loss=1.280, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "283: loss=1.281, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "284: loss=1.278, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "285: loss=1.277, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "286: loss=1.279, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "287: loss=1.278, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "288: loss=1.275, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "289: loss=1.277, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "290: loss=1.274, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "291: loss=1.274, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "292: loss=1.268, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "293: loss=1.270, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "294: loss=1.270, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "295: loss=1.266, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "296: loss=1.268, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "297: loss=1.258, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "298: loss=1.262, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "299: loss=1.259, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "300: loss=1.260, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "301: loss=1.259, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "302: loss=1.258, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "303: loss=1.263, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "304: loss=1.258, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "305: loss=1.258, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "306: loss=1.259, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "307: loss=1.255, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "308: loss=1.253, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "309: loss=1.253, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "310: loss=1.254, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "311: loss=1.255, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "312: loss=1.254, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "313: loss=1.257, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "314: loss=1.250, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "315: loss=1.249, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "316: loss=1.246, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "317: loss=1.244, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "318: loss=1.246, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "319: loss=1.243, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "320: loss=1.250, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "321: loss=1.245, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "322: loss=1.248, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "323: loss=1.246, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "324: loss=1.241, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "325: loss=1.239, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "326: loss=1.237, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "327: loss=1.244, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "328: loss=1.243, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "329: loss=1.243, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "330: loss=1.244, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "331: loss=1.246, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "332: loss=1.253, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "333: loss=1.247, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "334: loss=1.248, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "335: loss=1.249, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "336: loss=1.253, reward_mean=0.003, reward_bound=0.000\n",
      "500\n",
      "337: loss=1.253, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "338: loss=1.246, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "339: loss=1.253, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "340: loss=1.253, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "341: loss=1.257, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "342: loss=1.260, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "343: loss=1.265, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "344: loss=1.266, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "345: loss=1.260, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "346: loss=1.258, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "347: loss=1.256, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "348: loss=1.254, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "349: loss=1.251, reward_mean=0.000, reward_bound=0.000\n",
      "500\n",
      "350: loss=1.252, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "351: loss=1.254, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "352: loss=1.259, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "353: loss=1.260, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "354: loss=1.261, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "355: loss=1.260, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "356: loss=1.266, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "357: loss=1.266, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "358: loss=1.262, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "359: loss=1.264, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "360: loss=1.264, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "361: loss=1.266, reward_mean=0.002, reward_bound=0.000\n",
      "500\n",
      "362: loss=1.264, reward_mean=0.001, reward_bound=0.000\n",
      "500\n",
      "363: loss=1.268, reward_mean=0.001, reward_bound=0.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-3f0d1f9b7f44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDiscreteOneHotWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.79\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-132-b262b6edabad>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, target_reward, lr)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0melite_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0miter_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterate_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miter_no\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No convergence!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-f199ec940ddd>\u001b[0m in \u001b[0;36miterate_batches\u001b[1;34m(env, net, batch_size)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m#print(\"action:\",action)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;31m#discounting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mGAMMA\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\git\\gym\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-112-d013e5669ee6>\u001b[0m in \u001b[0;36mobservation\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(a, order)\u001b[0m\n\u001b[0;32m   1503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m     \"\"\"\n\u001b[1;32m-> 1505\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1507\u001b[0m \u001b[1;31m# Basic operations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(is_slippery=False)\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
    "env = DiscreteOneHotWrapper(env)\n",
    "\n",
    "train(env, 0.79, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
