{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py36\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "from collections import namedtuple, defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "from torch import FloatTensor, LongTensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "ENV_NAME_LAKE_SMALL =  \"FrozenLake-v0\"\n",
    "ENV_NAME_LAKE_BIG =\"FrozenLake8x8-v0\"\n",
    "\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, envname):\n",
    "        self.env = gym.make(envname)\n",
    "        self.state = self.env.reset()\n",
    "        # rewards per (state, action, new_state) \n",
    "        self.rewards = defaultdict(float)\n",
    "        # count transtions from each (state,action)\n",
    "        self.transits = defaultdict(Counter)\n",
    "        # calculated value per state in Value iteration and \n",
    "        # value per (state, action) in Q iteration\n",
    "        self.values = defaultdict(float)\n",
    "\n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            \n",
    "            self.state = self.env.reset() if is_done else new_state\n",
    "     \n",
    "    \n",
    "    # play one episode untill done\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "    \n",
    "    # select max of possible actions from this state\n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "class AgentV(Agent):\n",
    "    def __init__(self, envname):\n",
    "        super().__init__(envname)\n",
    "\n",
    "    def calc_action_value(self, state, action):\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        for tgt_state, count in target_counts.items():\n",
    "            reward = self.rewards[(state, action, tgt_state)]\n",
    "            action_value += (count / total) * (reward + GAMMA * self.values[tgt_state])\n",
    "        return action_value       \n",
    "    \n",
    "    # one value iteration for all states\n",
    "    def iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [self.calc_action_value(state, action)\n",
    "                            for action in range(self.env.action_space.n)]\n",
    "            self.values[state] = max(state_values)\n",
    "    \n",
    "class AgentQ(Agent):\n",
    "    def __init__(self, envname):\n",
    "        super().__init__(envname)\n",
    "\n",
    "    # in Q case , Q value is already stored\n",
    "    def calc_action_value(self, state, action):\n",
    "        return self.values[(state, action)]\n",
    "    \n",
    "    # one Q iteration for all ( state, action ) pairs\n",
    "    def iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            for action in range(self.env.action_space.n):\n",
    "                action_value = 0.0\n",
    "                target_counts = self.transits[(state, action)]\n",
    "                total = sum(target_counts.values())\n",
    "                for tgt_state, count in target_counts.items():\n",
    "                    reward = self.rewards[(state, action, tgt_state)]\n",
    "                    best_action = self.select_action(tgt_state)\n",
    "                    action_value += (count / total) * (reward + GAMMA * self.values[(tgt_state, best_action)])\n",
    "                self.values[(state, action)] = action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(envname, v_or_q = \"v\"):\n",
    "    test_env = gym.make(envname)\n",
    "    if v_or_q == \"v\":\n",
    "        agent = AgentV(envname)\n",
    "    else:\n",
    "        agent = AgentQ(envname)\n",
    "    writer = SummaryWriter(comment= \"-%s-iteration\" % (v_or_q))\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        agent.play_n_random_steps(100)\n",
    "        agent.iteration()\n",
    "            \n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        if reward > best_reward:\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "            best_reward = reward\n",
    "        if reward > 0.80:\n",
    "            print(\"Solved in %d iterations!\" % iter_no)\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMALL FROZEN LAKE => VALUE ITERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.500\n",
      "Best reward updated 0.500 -> 0.800\n",
      "Best reward updated 0.800 -> 0.850\n",
      "Solved in 39 iterations!\n"
     ]
    }
   ],
   "source": [
    "train(ENV_NAME_LAKE_SMALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIG FROZEN LAKE => VALUE ITERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.350\n",
      "Best reward updated 0.350 -> 0.400\n",
      "Best reward updated 0.400 -> 0.500\n",
      "Best reward updated 0.500 -> 0.650\n",
      "Best reward updated 0.650 -> 0.850\n",
      "Solved in 268 iterations!\n"
     ]
    }
   ],
   "source": [
    "train(ENV_NAME_LAKE_BIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMALL FROZEN LAKE => Q ITERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.100\n",
      "Best reward updated 0.100 -> 0.400\n",
      "Best reward updated 0.400 -> 0.600\n",
      "Best reward updated 0.600 -> 0.750\n",
      "Best reward updated 0.750 -> 0.900\n",
      "Solved in 15 iterations!\n"
     ]
    }
   ],
   "source": [
    "train(ENV_NAME_LAKE_SMALL, v_or_q=\"q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIG FROZEN LAKE => Q ITERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.050\n",
      "Best reward updated 0.050 -> 0.200\n",
      "Best reward updated 0.200 -> 0.350\n",
      "Best reward updated 0.350 -> 0.500\n",
      "Best reward updated 0.500 -> 0.550\n",
      "Best reward updated 0.550 -> 0.650\n",
      "Best reward updated 0.650 -> 0.700\n",
      "Best reward updated 0.700 -> 0.800\n",
      "Best reward updated 0.800 -> 0.850\n",
      "Solved in 438 iterations!\n"
     ]
    }
   ],
   "source": [
    "train(ENV_NAME_LAKE_BIG, v_or_q=\"q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
